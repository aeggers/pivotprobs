---
title: "Comparing integration methods"
author: "Andy Eggers"
date: "8/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(rgl.useNULL=TRUE)
library(rgl)
library(tidyverse)
# library(dplyr)
library(knitr)
library(kableExtra)
knitr::knit_hooks$set(webgl = hook_webgl)

```

## Objective 

We want to check that we get the same results via Monte Carlo and numerical integration methods for some increasingly complicated problems. 

### Background 

#### Normalizing constant for Monte Carlo simulations 

First, let's consider how we estimate pivot probabilities using simulation. All pivot probabilities involve an equation and possibly some inequalities; for example, a tie for first between $a$ and $b$ takes place when $v_a > v_c$ and $v_b > v_c$ and $v_a = v_b$. The equality of course never holds exactly in a set of simulated results with continuous vote shares, so we count the proportion of cases that fall within some window of equality, e.g. those where $|v_a  - v_b| < \varepsilon/2$. We then need to account for the size of the window we chose, i.e. $\varepsilon$. I had done this by dividing by $\varepsilon$. It turns out that this is not quite correct, and that there is no simple fix that applies across all cases we want to study.   

For example, consider estimating the probability that $v_1 = .25$. We could approximate this by drawing a large number of simulated elections and computing the proportion where $|v_1 - .25| < \varepsilon/2$ or the proportion where $0 < v_1 - .25 < \varepsilon$; the result will clearly depend on $\varepsilon$, and it seemed reasonable at first to divide by $\varepsilon$ to approximate the integral because the width of the "target area" (and therefore the area) is a linear function of $\varepsilon$. But consider a horizontal band on the unit simplex containing the points where $v_1$ is between $a - \frac{\varepsilon}{2}$ and $a + \frac{\varepsilon}{2}$. Two points across from each other on this band would be $\left(a + \frac{\varepsilon}{2}, \frac{1 - \left( a  + \frac{\varepsilon}{2}\right)}{2}, \frac{1 - \left( a  + \frac{\varepsilon}{2}\right)}{2}\right)$ and  $\left(a - \frac{\varepsilon}{2}, \frac{1 - \left( a  - \frac{\varepsilon}{2}\right)}{2}, \frac{1 - \left( a  - \frac{\varepsilon}{2}\right)}{2}\right)$. The distance between these points is 
$\sqrt{\varepsilon^2 + \frac{\varepsilon^2}{4} + \frac{\varepsilon^2}{4}} = \sqrt{\frac{3}{2}}\varepsilon$. So it seems that the "target area" increases by $\sqrt{\frac{3}{2}}$ units for every unit of $\varepsilon$. Intuitively, if we think of the simplex as flat, then we will underestimate the distance between points: as we move toward the top vertex, we are actually going uphill, which means a longer journey than if it were flat. 

When we compute a tie for first between two candidates, by contrast, the proportion in the window increases by $\sqrt{2}$. To see this, note that two points on either side of the channel where candidates 1 and 2 get $x$ would be $(x - \varepsilon/2, x + \varepsilon/2, 1-2x)$ and $(x + \varepsilon/2, x - \varepsilon/2, 1-2x)$, and the distance between those two points is $\sqrt{\frac{\varepsilon^2}{2}} = \frac{\varepsilon}{\sqrt{2}}$

We could go through all the cases we need to deal with, but this gets pretty challenging in high dimensions. 

I can't think of a general solution to this. I thought I could estimate the probability for various values of $\varepsilon$ and extrapolate to where $\varepsilon = 0$, but that isn't correct (the intercept must be 0); extrapolating the values divided by $\varepsilon$ to where $\varepsilon = 0$ also sounds promising but does not address the issue. The issue, put differently, is that the probability of being within a window of $\varepsilon$ is $p \times C$, where $p$ is the pivot probability and $C$ is an unknown constant related to the volume of the polyhedron associated with a window of $\varepsilon$. So below I will just report results divided by $\varepsilon$, which will differ from the true results by a constant amount. I will need to keep thinking about a general solution. 



<!-- Instead, I propose to have a general approach to computing pivot probabilities by simulation, which is to get estimates at a few values of $\varepsilon$ and use linear regression to estimate the limit of these estimates divided by $\varepsilon$ where $\varepsilon = 0$.  -->

<!-- As an example, let's do the probability that $v_1 = .25$, the example above. Here we know what the normalizing constant should be, so we can compute the value. -->

<!-- ```{r sim_approach_example, cache = T} -->
<!-- sim_func <- function(sims, tol, a = .25){ -->
<!--   mean(abs(sims[,1] - a) < tol/2) -->
<!-- } -->

<!-- N <- 1000000 -->
<!-- sims <- gtools::rdirichlet(N, alpha = rep(1,3)) -->
<!-- tibble(tol = seq(.005, .03, by = .005)) %>%  -->
<!--   mutate(piw = pmap(., sim_func, sims = sims) %>% as.numeric()) -> df  -->

<!-- df %>%  -->
<!--   mutate(norm_piw = piw/tol) %>%  -->
<!--   ggplot(aes(x = tol, y = norm_piw)) +  -->
<!--   geom_point() -->
<!-- ``` -->

<!-- Or how about case of a tie between two candidates:  -->

<!-- ```{r sim_approach_example_2, cache = T} -->
<!-- sim_func_tie <- function(sims, tol){ -->
<!--   mean(abs(sims[,1] - sims[,2]) < tol/2) -->
<!-- } -->

<!-- tibble(tol = seq(.005, .03, by = .005)) %>%  -->
<!--   mutate(piw = pmap(., sim_func_tie, sims = sims) %>% as.numeric()) -> df2  -->

<!-- df2 %>%  -->
<!--   mutate(norm_piw = piw/tol) %>%  -->
<!--   ggplot(aes(x = tol, y = norm_piw)) +  -->
<!--   geom_point() -->
<!-- ``` -->


<!-- ```{r checkit, eval=F} -->
<!-- pp <- lm(I(piw/tol) ~ tol, data = df) %>% broom::tidy() %>% filter(term == "(Intercept)") %>% pull(estimate) -->

<!-- df %>% mutate(pp = piw/(tol*sqrt(3/2))) -->

<!-- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(.25, .75, 0), c(.25, 0, .75)), alpha = rep(1,3)) -->
<!-- ``` -->


<!-- Start with a problem in three dimensions. We will look at the relationship between the probability of a result being within $\varepsilon$ of some condition and the value of $\varepsilon$. As we increase $\varepsilon$ the probability should increase. If it goes up linearly, then we can approximate the integral (i.e. the limit of the probability of being in the window as $\varepsilon$ goes to zero) by dividing by $\varepsilon$. -->



<!-- The question is how we can detect this issue. I thought we could detect it by looking at how the probability of being within $\varepsilon$ depended on $\varepsilon$. But note that this derivative is actually the pivot probability (which is unknown) times the derivative of the "width" of the target area with respect to $\varepsilon$. So in general this will not work. We can of course compare to the analytical solution.  -->

<!-- It's discouraging, though, because it appears that the normalizing constant depends on the question in intricate ways. For example, consider the event that $v_1 = b v_2$. Then I think the normalizing constant is $\frac{\sqrt{2}}{b} \varepsilon$. Thinking about the various complicated pivot probabilities we have (positional, Kemeny, etc) it would be annoying to work out all the normalizing constants.     -->

<!-- So as a first step we want to make sure that our simulation-based results are linearly related to our analytical result. If we can work out the normalizing constant, great, we expect the results to be exactly the same. Otherwise, we can try to work it out or be content that the simulation is off by a fixed multiplicative factor, and we will use the analytical results.  -->



#### Normalizing constant for Dirichlet distribution 

I would have expected the Dirichlet distribution to integrate to 1 over the whole unit simplex. But this is not the case. Instead, the integral over the whole unit simplex is $\sqrt{d}$, where $d$ is the number of dimensions:  

```{r unity_checks, cache = T}
# the Dirichlet with no correction
dir_fnx <- function(x, alpha){
  gtools::ddirichlet(as.vector(x), alpha = alpha)
}

integral_over_unit_simplex <- function(d){
  SimplicialCubature::adaptIntegrateSimplex(dir_fnx, S = diag(d), alpha = rep(1, d))$integral
}

tibble(d = 2:7) %>% 
  mutate(integral = pmap(., integral_over_unit_simplex)) %>% 
  unnest(cols = c(integral)) -> integrals 
integrals

tibble(d = seq(2, 7, by = .1)) %>% 
  mutate(integral = sqrt(d)) -> df2 
df2 
```

```{r sep_for_debug}
integrals %>% 
  ggplot(aes(x = d, y = integral)) + 
  geom_point() + 
  geom_line(data = df2, aes(x = d, y = integral), col = "blue") 
```

This implies that we need to apply a correction that divides the Dirichlet density by $\sqrt{d}$. 



### Problem 1 

We start by computing the probability of a tie for first between two candidates in a three-candidate plurality contest. 

For the integrand function, we assume that results are distributed according to $\text{Dirichlet}(10, 8, 6)$. 

```{r params}
alpha = c(10, 8, 6)
```


#### Direct Monte Carlo

We draw a large number of simulated elections and see how often the first two candidates nearly tie: 

```{r dmc, cache = T}
set.seed(12345)
N <- 1000000
sims <- gtools::rdirichlet(N, alpha)
tol <- .01 # window defining a tie
dmc_result <- mean(sims[,1] > sims[,3] & #1  beats 3
                     sims[,2] > sims[,3] & # 2 beats 3 
                     abs(sims[,1] - sims[,2]) < tol/2 # 1 and 2 are within tol
                   )/tol # actually we should normalize by sqrt(2)  
dmc_result 
```

Note that a "near tie" here means 1 and 2 being within `tol` of each other. We then divide by `tol`, though the correct normalizer is `tol/sqrt(2)`, which is the width of the channel that is being considered a close election. We divide by this width because we are interested in the density at a single width-0 strip of this channel. The width of the channel is `tol/sqrt(2)` (not `tol`, as I initially thought) because two points on either side of the channel where candidates 1 and 2 get $x$ would be $(x - a/2, x + a/2, 1-2x)$ and $(x + a/2, x - a/2, 1-2x)$, and the distance between those two points is $\sqrt{\frac{a^2}{2}} = \frac{a}{\sqrt{2}}$.   

#### Numerical integration via regular grid 

We integrate the integrand function at grid points along the line between $(1/3, 1/3, 0)$ and $(1/2, 1/2, 0)$:

```{r nivg, cache = T}
n_midpoints <- 1000
breakpoints <- seq(from = 1/3, to = 1/2, length = n_midpoints + 1)
midpoints <- breakpoints[-length(breakpoints)] + (breakpoints[2] - breakpoints[1])/2
grid <- cbind(midpoints, midpoints, 1 - 2*midpoints)
segment_length <- sqrt(sum((grid[2,] - grid[1,])^2))
fn_values <- gtools::ddirichlet(grid, alpha = alpha)/sqrt(3)
nigv_result <- sum(fn_values)*segment_length
nigv_result
```

This agrees with the simulation result after the correction is applied. Note the $\sqrt{3}$ correction to the Dirichlet density, which is explained above. 

<!-- . This is because the integral over the entire $d$-dimensional simplex is $\sqrt{d}$ (see below) while the proportion of simulation draws on the unit simplex is always 1.  -->
<!-- It seems to me an error in the definition of the Dirichlet density if, when  integrated over the entire unit simplex, it returns $\sqrt{d}$. So I add that correction.  -->


#### Numerical integration via `SimplicialCubature`  

The function we want to integrate is this: 

```{r dir_fn_with_fix, error = T}
dir_fn <- function(x, alpha){
  gtools::ddirichlet(x = as.vector(x), alpha = alpha)/sqrt(length(alpha))
}
```

Note the $\sqrt{3}$ correction that I also used for the manual numerical integration above. (See below why I write `as.vector(x)`, which suggests a possible fix for John's code.)

To apply `SimplicialCubature`, we need to define the `S` matrix defining the simplex or simplices over which we will be integrating. The `S` matrix has a column for each simplex vertex. 

First we confirm that we get 1 when we integrate over the whole unit simplex: 
```{r confirm_1, cache = T} 
confirm_1 <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = diag(3), alpha = alpha)
confirm_1
```

Now we seek to compute the probability of a tie between candidates 1 and 2. The `S` matrix in this case has two columns identifying the points on the unit simplex between which we want to integrate: 

```{r define_S} 
S <- cbind(c(1/2, 1/2, 0), c(1/3, 1/3, 1/3))
S
```

The integral is then:
```{r sc, cache = T, error = T}
# ch
sc_result <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = S, alpha = alpha)
sc_result$integral
```

#### Comparison 

```{r comp_table_1}
tibble(`Tie between` = c("ab"), `Monte Carlo` = dmc_result,
                 `Monte Carlo normalized` = dmc_result*sqrt(2), 
                 `Manual integration` = nigv_result, 
                 `SimpCub integration` = sc_result$integral) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```

### Problem 2

Get the probability of all three ties for first given the same `alpha` vector. 

#### Direct Monte Carlo

First we define a function: 

```{r three_sims_func}
ab_tie_for_first_MC <- function(sims, tol = .01){
  row_max <- apply(sims, 1, max)
  mean((sims[,1] == row_max | sims[,2] == row_max) & abs(sims[,1] - sims[,2]) < tol/2)/tol  
}
```

Then we do the computations: 
```{r three_sims_cal, cache = T}
# ch
dmc_3 <- c(ab_tie_for_first_MC(sims), ab_tie_for_first_MC(sims[,c(1,3,2)]), ab_tie_for_first_MC(sims[,c(2,3,1)]))
dmc_3
```

#### Numerical integration via grid 

First we define a function:

```{r nivg_x, cache = T}
ab_tie_for_first_nivg3 <- function(alpha, n_midpoints = 1000){
  breakpoints <- seq(from = 1/3, to = 1/2, length = n_midpoints + 1)
  midpoints <- breakpoints[-length(breakpoints)] + (breakpoints[2] - breakpoints[1])/2
  grid <- cbind(midpoints, midpoints, 1 - 2*midpoints)
  segment_length <- sqrt(sum((grid[2,] - grid[1,])^2))
  fn_values <- gtools::ddirichlet(grid, alpha = alpha)/sqrt(3)
  sum(fn_values)*segment_length
}
```

Then we do the computations: 
```{r three_nivg_cal}
nivg_3 <- c(ab_tie_for_first_nivg3(alpha), ab_tie_for_first_nivg3(alpha[c(1,3,2)]), ab_tie_for_first_nivg3(alpha[c(2,3,1)]))
nivg_3
```

#### Numerical integration via `SimplicialCubature`

```{r three_scs_x, cache = T}
#ch
sc_result_ab <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(.5, .5, 0), c(1/3, 1/3, 1/3)), alpha = alpha)
sc_result_ac <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(.5, 0, .5), c(1/3, 1/3, 1/3)), alpha = alpha)
sc_result_bc <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(.0, .5, .5), c(1/3, 1/3, 1/3)), alpha = alpha)

sc_3 <- c(sc_result_ab$integral, sc_result_ac$integral, sc_result_bc$integral)
sc_3
```

#### Comparison 

```{r comp_table_2}
df <- tibble(`Tie between` = c("ab", "ac", "bc"), `Monte Carlo` = dmc_3, `Monte Carlo normalized` = dmc_3*sqrt(2),
               `Manual integration` = nivg_3, `SimpCub integration` = sc_3)
kable(df) %>% 
  kable_styling(full_width = F)
```


### Problem 3

What is the probability of each candidate receiving at least 50\% of the vote, given the same `alpha` parameters? 


#### Direct Monte Carlo

Here there are no normalization issues with the Monte Carlo.

```{r sims_more_than_50, cache = T}
a_over_50 <- mean(sims[,1] > .5)
b_over_50 <- mean(sims[,2] > .5)
c_over_50 <- mean(sims[,3] > .5)
```

#### `SimplicialCubature`

```{r sc_50, cache = T, error = T}
#ch
S_50 <- cbind(c(.5, .5, 0), c(.5, 0, .5), c(1,0,0))
a_over_50_sc <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = S_50, alpha = alpha)
b_over_50_sc <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = S_50, alpha = alpha[c(2,1,3)])
c_over_50_sc <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = S_50, alpha = alpha[c(3,1,2)])
```

#### Comparison 

```{r comp_table_3}
df <- tibble(`Candidate` = c("a", "b", "c"), `Monte Carlo` = c(a_over_50, b_over_50, c_over_50), `SimpCub integration` = c(a_over_50_sc$integral, b_over_50_sc$integral, c_over_50_sc$integral))
kable(df) %>% 
  kable_styling(full_width = F)
```


### Problem 4

Compute the probability of each tie for first in a four-candidate plurality race with results distributed according to $\text{Dirichlet}(10, 8, 6, 4)$.

```{r sto_alpha4}
alpha4 <- c(10, 8, 6, 4)
```

#### Direct Monte Carlo

First define a function for cycling through cases: 

```{r dmc_func}
plurality_tie_probs_from_sims <- function(sims, tol = .01, sep = ""){

  out <- list()
  cand_names <- letters[1:ncol(sims)]
  for(i in 1:(ncol(sims)-1)){
    for(j in (i+1):ncol(sims)){
      out[[paste0(cand_names[i], sep, cand_names[j])]] = ab_tie_for_first_MC(cbind(sims[,c(i,j)], sims[,-c(i,j)])) 
    }
  }
  out
  
}
```

Then generate the simulations and apply the function: 

```{r dmc_4, cache = T}
# ch
N <- 10000000
sims4 <- gtools::rdirichlet(N, alpha4)
dmc_4 <- plurality_tie_probs_from_sims(sims4)

```

#### `SimplicialCubature` approach 

To compute the probability of a tie between $a$ and $b$, we need to integrate over a quadrilateral facet with the vertices $(1/2, 1/2, 0, 0)$, $(1/3, 1/3, 1/3, 0)$, $(1/3, 1/3, 0, 1/3)$, $(1/4, 1/4, 1/4, 1/4)$. 

`SimplicialCubature` requires the area of integration to be specified as a set of simplices. We do this by creating a matrix for each simplex and combining them as an array. 

So in this case `S` is: 

```{r S4_plurality}
v_ab <- c(1/2, 1/2, 0, 0)
v_abc <- c(1/3, 1/3, 1/3, 0)
v_abd <- c(1/3, 1/3, 0, 1/3)
v_abcd <- c(1/4, 1/4, 1/4, 1/4)
S4 <- array(c(cbind(v_ab, v_abc, v_abd), cbind(v_abc, v_abd, v_abcd)), dim = c(4,3,2))

```

And we can write a function to apply it repeatedly: 

```{r sc_func}
plurality_tie_probs_from_sc <- function(alpha, S, sep = "", ...){

  out <- list()
  cand_names <- letters[1:length(alpha)]
  for(i in 1:(length(cand_names)-1)){
    for(j in (i+1):length(cand_names)){
      out[[paste0(cand_names[i], sep, cand_names[j])]] = SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = S, alpha = c(alpha[c(i,j)], alpha[-c(i,j)]), ...)$integral  # throwing out other components for now 
    }
  }
  out  
  
}
```

And apply it: 

```{r sc_4, cache = T, eval = T}
sc_4 <- plurality_tie_probs_from_sc(alpha4, S = S4)
```


#### Comparison

```{r comp_table_4}
tibble(`Candidate pair` = names(sc_4), 
          `Monte Carlo` = unlist(dmc_4), 
          `Monte Carlo normalized` = unlist(dmc_4)*sqrt(2),
          `SimpCub integration` = unlist(sc_4)) %>% 
  mutate(Ratio = `SimpCub integration`/`Monte Carlo normalized`) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```





## Extending `SimplicialCubature`

Okay, here is the cool part. We want to get pivot probabilities for arbitrary voting systems. Having shown that we can use `SimplicialCubature` for the integration, we now require a way to get the `S` array for each kind of pivot event. My approach is to start from a matrix of "winning conditions", expressed as coefficients on inequalities relative to zero. For example, the conditions for winning in a four-candidate plurality contest could be written 

\begin{array}
v_1 - v_2 >=& 0 \\
v_1 -  v_3 >=& 0 \\
v_1 - v_4 >=& 0. \\
\end{array}

I represent these in matrix form as follows: 

```{r pwv4}
plurality_win_conditions_4 <- rbind(c(1,-1,0,0), c(1,0,-1,0), c(1,0,0,-1)) 
plurality_win_conditions_4
```

Each row states a condition $\beta$ such that $\mathbf{v}\beta \geq 0$ should hold when candidate 1 wins. Then the `cand_a_win_region_vertices_from_win_conditions()` function returns the vertices of candidate 1's "win region" given the matrix of win conditions, and `simplices_to_integrate_from_win_region_vertices()` takes these vertices plus a single win condition that must bind with equality and returns an `S` array representing the simplices on facets of candidate 1's win region where the given condition binds with equality. Putting this all together, 
`S_array_from_win_conditions()` takes a matrix of win conditions and returns the `S` array that describes the facets where the first win condition binds with equality. To illustrate with a four-candidate plurality contest:

```{r S_from_wc}
source("R/general_numerical_methods.R")
S_4ab <- S_array_from_win_conditions(plurality_win_conditions_4)
S_4ab
S_4ac <- S_array_from_win_conditions(plurality_win_conditions_4[c(2,1,3),])
S_4ac
```

Here we have Borda count: 

```{r borda_S_from_wc}
s <- 1/2
borda_count_win_conditions <- 
  rbind(c(1-s, 1, s - 1, -1, s, -s),
        c(1, 1-s, s, -s, s - 1, -1))
S_bc <- S_array_from_win_conditions(borda_count_win_conditions)
S_bc

```

Now we can test that we produce the right results when generating the `S` array from a matrix of win conditions. This allows us to go into higher-dimensional cases where we might otherwise struggle to generate the `S` array.  

### Problem 5

Compute the probability of each tie for first in a five-candidate plurality race with results distributed according to $\text{Dirichlet}(10, 8, 6, 4, 3)$.

#### Direct Monte Carlo

```{r problem_5_sims, cache = T}
# ch
alpha5 <- c(10, 8, 6, 4, 3)
sims5 <- gtools::rdirichlet(1000000, alpha5)
system.time(pp_sims_5 <- plurality_tie_probs_from_sims(sims5))
```

#### `SimplicialCubature` with generated `S` array

```{r problem_5_sc, cache = T}
wc5 <- cbind(rep(1,4), -diag(4))
S_5 <- S_array_from_win_conditions(wc5)
S_5
system.time(pp_sc_5 <- plurality_tie_probs_from_sc(alpha5, S_5))
```

#### Comparison 

```{r comparison_5}
tibble(`Candidate pair` = names(pp_sc_5), `Monte Carlo` = unlist(pp_sims_5),
       `Monte Carlo normalized` = unlist(pp_sims_5)*sqrt(2), `SimpCub integration` = unlist(pp_sc_5)) %>% 
  mutate(Ratio = `SimpCub integration`/`Monte Carlo normalized`) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```

So we reproduce the simulation results, but note that integration with the default settings takes a long time. We can speed it up a lot by relaxing the requirements about error. 

```{r problem_5_sc_more_error, cache = T}
wc5 <- cbind(rep(1,4), -diag(4))
S_5 <- S_array_from_win_conditions(wc5)
system.time(pp_sc_5_tol_1 <- plurality_tie_probs_from_sc(alpha5, S_5, tol = .1))
system.time(pp_sc_5_absError_01 <- plurality_tie_probs_from_sc(alpha5, S_5, absError = .01))

tibble(`Candidate pair` = names(pp_sc_5), `Monte Carlo` = unlist(pp_sims_5), `SimpCub integration` = unlist(pp_sc_5), `SimpCub integration tol .1` = unlist(pp_sc_5_tol_1), `SimpCub integration absError .01` = unlist(pp_sc_5_absError_01)) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```


### Problem 6

Compute the probability of each tie for first in a three-candidate Borda count contest with results (described by the proportion of ballots with rank ordering $abc, acb, bac, bca, cab, cba$) distributed according to $\text{Dirichlet}(10, 6, 4, 5, 3, 11)$ .

#### Direct Monte Carlo

I used some simulations to try to work out the normalization factor for Borda Count, but I think now this didn't make sense. (I'm not sure why it gives the right answer.) I leave it here for later investigation.

##### Possibly incorrect normalization procedure


In the plurality case, we established that the target area increased by $\sqrt{2}$ for each increase in the width of the window we used to locate ties for first. Some initial testing indicated a similar issue comes up with Borda count when we look for two scores to be tied for first, but that $\sqrt{2}$ is not the correct factor. To find the correct factor without doing any serious geometry, we do the following experiment: for a fixed set of simulations, how much does the probability of two scores being within `tol` increase as we vary `tol`? In the plurality case it should have increased by approximately $\sqrt{2}$. What is it here? 

```{r fix_sims, cache = T}
# draw simulations 
bc_alpha <- c(10, 6, 4, 5, 3, 11)
sims_bc <- gtools::rdirichlet(1000000, alpha = bc_alpha)

score_mat <- cbind(sims_bc[,1] + sims_bc[,2] + s*(sims_bc[,3] + sims_bc[,5]), 
                   sims_bc[,3] + sims_bc[,4] + s*(sims_bc[,1] + sims_bc[,6]),
                   sims_bc[,5] + sims_bc[,6] + s*(sims_bc[,2] + sims_bc[,4]))

# function for getting the probability of being within a window defined by tol
pr_in_window <- function(tol, sims){
  mean(sims[,1] > sims[,3] & sims[,2] > sims[,3] & abs(sims[,1] - sims[,2]) < tol/2)
}

# apply that function across several values of tol
data.frame(tol = seq(.005, .03, by = .005)) %>% 
  mutate(piw = as.numeric(pmap(., pr_in_window, sims = score_mat))) -> df 

# plot the result along with a line of slope 1 and a line of slope sqrt(3)
df %>% ggplot(aes(x = tol, y = piw)) + geom_point() + geom_abline(slope = 1, intercept = 0, linetype = 2) + geom_abline(slope = sqrt(3), intercept = 0, linetype = 2, col = "orange")

```

So the correction factor is $\sqrt{3}$. I'm not sure, though, that this really makes sense: the slope is surely the pivot probability times the normalizing factor, i.e. it would only go up with slope 1 if the pivot probability was 1. But perhaps I am missing something.  

##### Resuming main analysis 

We apply that in the function `positional_piv_probs_simulation()`, which takes a set of simulations (6 ballot shares) and a value of `s` (Borda has `s = .5`) and returns the three pivot probabilities. (I draw a new sample so that the time comparison is more appropriate.)

```{r bc_sims, cache = T}
# ch
source("R/positional.R")
system.time(pp_bc <- positional_piv_probs_simulation(
  gtools::rdirichlet(1000000, alpha = bc_alpha))
)
```

#### `SimplicialCubature` with generated `S` array

`ordinal_shuffle_dirichlet_pivot_probs()` takes an `alpha` vector and an `S` array and integrates the Dirichlet distribution with parameter vector `alpha` over `S` for each pair of candidates. `S` corresponds to the tie between candidates 1 and 2. To get the tie between 1 and 3, it shuffles the `alpha` vector so that 3 and 2 switch places, i.e. `alpha` is replaced by `alpha[c(2,1,5,6,3,4)]`.  

```{r sc_borda, cache = T}
system.time(pp_sc_bc <- ordinal_shuffle_dirichlet_pivot_probs(bc_alpha, S_bc))
system.time(pp_sc_bc_tol_1 <- ordinal_shuffle_dirichlet_pivot_probs(bc_alpha, S_bc, tol = .1))
system.time(pp_sc_bc_absError_01 <- ordinal_shuffle_dirichlet_pivot_probs(bc_alpha, S_bc, absError = .01))
```

The `tol` and `absError` arguments don't seem to have any effect on speed here, nor do they affect the results. Is it because they didn't complete the computations? Yes.

```{r report_bc_sc}
c(pp_sc_bc$ab$message, pp_sc_bc_tol_1$ab$message, pp_sc_bc_absError_01$ab$message)
c(pp_sc_bc$ab$functionEvaluations, pp_sc_bc_tol_1$ab$functionEvaluations, pp_sc_bc_absError_01$ab$functionEvaluations)
```

So now I increase the maximum number of function evaluations: 

```{r sc_borda_2, cache = T}
maxEvals <- 100000
system.time(pp_sc_bc <- ordinal_shuffle_dirichlet_pivot_probs(bc_alpha, S_bc, maxEvals = maxEvals))
system.time(pp_sc_bc_tol_1 <- ordinal_shuffle_dirichlet_pivot_probs(bc_alpha, S_bc, tol = .1, maxEvals = maxEvals))
system.time(pp_sc_bc_absError_01 <- ordinal_shuffle_dirichlet_pivot_probs(bc_alpha, S_bc, absError = .01, maxEvals = maxEvals))
```

And see the report: 
```{r report_bc_sc_2}
tibble(`Candidate pair` = c("ab", "ac", "bc"), 
       `Default` = c(pp_sc_bc$ab$functionEvaluations, pp_sc_bc$ac$functionEvaluations, pp_sc_bc$bc$functionEvaluations),
       `tol = .01` = c(pp_sc_bc_tol_1$ab$functionEvaluations, pp_sc_bc_tol_1$ac$functionEvaluations, pp_sc_bc_tol_1$bc$functionEvaluations),
       `absError = .01` = c(pp_sc_bc_absError_01$ab$functionEvaluations, pp_sc_bc_absError_01$ac$functionEvaluations, pp_sc_bc_absError_01$bc$functionEvaluations)) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```


#### Comparison 

```{r bc_comparison}

tibble(`Candidate pair` = names(pp_sc_bc), `Monte Carlo` = unlist(pp_bc), `SimpCub integration` = c(pp_sc_bc$ab$integral, pp_sc_bc$ac$integral, pp_sc_bc$bc$integral), `SimpCub integration tol .1` = c(pp_sc_bc_tol_1$ab$integral, pp_sc_bc_tol_1$ac$integral, pp_sc_bc_tol_1$bc$integral), `SimpCub integration absError .01` = c(pp_sc_bc_absError_01$ab$integral, pp_sc_bc_absError_01$ac$integral, pp_sc_bc_absError_01$bc$integral)) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```

Conclusion: the method matches the simulation results (up to a normalization factor), but it takes a large number of function evaluations so it is slow. It can be sped up by specifying `tol = .1`. If we don't care about normalization, we might prefer the faster simulation approach. But we can think of cases where it is important to have some precision on a very low probability event to make the polling algorithm work correctly. Suppose candidate $a$ is way ahead of the other two, so that all of the pivot probabilities are really low, and consider a voter who strongly prefers $a$ to $b$ and $c$. The strategic move is to rank the likely second-place candidate last, and this could depend on random variation in the simulations. You could argue that we don't really care what happens in that case, but it's good to have a more complete solution. 

I wonder how the numerical result varies with `tol`. 

```{r tol_test, cache = T}
bc_alpha_2 <- c(10, 8, 1.5,3, 2,3)
maxEvals <- 200000 # set this high so it doesn't bind

data.frame(tol = c(.0001, .001, .01, .1, .2, .3)) %>% 
  mutate(sc_result = pmap(., ordinal_shuffle_dirichlet_pivot_probs, alpha = bc_alpha_2, S = S_bc, maxEvals = maxEvals)) %>% 
  unnest_longer(sc_result) %>% 
  rename(pp = sc_result_id) %>% 
  unnest_longer(sc_result) %>% 
  filter(sc_result_id %in% c("integral", "functionEvaluations")) %>% 
  unnest(sc_result) -> df2
```

```{r tol_plot, cache = T}
df2 %>% filter(sc_result_id == "integral") %>% 
  mutate(tolf = factor(tol)) %>% 
  ggplot(aes(x = tolf, y = sc_result, col = pp, group = pp)) + 
  scale_y_log10() + 
  geom_line() + 
  geom_point() + 
  labs(x = "tol argument to SC", y = "Pivot probability")
  

df2 %>% filter(sc_result_id == "functionEvaluations") %>% 
  mutate(tolf = factor(tol)) %>% 
  ggplot(aes(x = tolf, y = sc_result, col = pp, group = pp)) + 
  geom_line() + 
  geom_point() + 
  labs(x = "tol argument to SC", y = "Number of function evaluations")

```

This suggests that with pretty high `tol` we could do fine. It's tricky to figure out which is actually better as we don't know the circumstances we will be in.  


### Checking positional methods 

Let's do a large number of comparisons.

To set this up I want a list of alphas. 

```{r positional_comparisons}
set.seed(12345)
J <- 20
v_mat <- gtools::rdirichlet(J, alpha = c(4,1, 2, 2, 2, 5))
alpha_mat <- matrix(.8, nrow = J, ncol = 6) + v_mat*30

# I want a list of alpha arguments 
alpha_list <- as.list(as.data.frame(t(alpha_mat)))

# getting a dataframe of simulated elections for each alpha vector 
tibble(
  j = 1:length(alpha_list), 
  n = 1000000, 
  alpha = alpha_list) %>% 
  mutate(sims = pmap({.} %>% select(-j), gtools::rdirichlet)) -> sim_tibble

# note that {.} is because of this error -- magrittr creates an anonymous function.  https://stackoverflow.com/questions/50729045/meaning-of-error-using-shorthand-inside-dplyr-function
# is . otherwise a dplyr thing, or magrittr? 

# for each simulation dataframe, and each positional s, get the simulation-based estimates  
expand_grid(s = c(0, 1/4, 1/2, 3/4, 1), sim_tibble) %>% 
  mutate(sim_based = pmap({.} %>% select(sims, alpha, s), positional_piv_probs_simulation)) -> sim_df 

expand_grid(s = c(0, 1/4, 1/2, 3/4, 1), sim_tibble) %>%
  mutate(sc_based_01 = pmap({.} %>% select(alpha, s), positional_pivot_probs_general, tol = .1, maxEvals = 30000, report = T)) -> sc_df 




# got an error here: 
#   Error in if ((NMRL > SMALL * NMBS) & (K < RLS)) { : 
#   missing value where TRUE/FALSE needed
# I wonder if that's the rational arithmetic issue. 

sim_df %>% 
  select(-sims) %>%
  unnest_longer(sim_based) %>% 
  rename(pp = sim_based_id,
         value = sim_based) -> sim_pps

sc_df %>% 
  select(-sims, -n) %>%
  unnest_longer(sc_based_01) %>% 
  rename(pp = sc_based_01_id) %>% 
  unnest_longer(sc_based_01) -> sc_pps

sc_pps %>% filter(sc_based_01_id == "message") %>% 
  unnest(sc_based_01) -> sc_messages 

sc_pps %>% filter(sc_based_01_id == "integral") %>% 
  unnest(sc_based_01) %>% 
  rename(value = sc_based_01) %>% 
  select(-sc_based_01_id) -> sc_pps_2

sc_pps_2 %>% 
  left_join(sim_pps, by = c("s", "j", "pp"), suffix = c("_cubature", "_monte_carlo")) -> df_for_plot


df_for_plot %>% 
  ggplot(aes(x = value_monte_carlo, y = value_cubature, col = factor(s))) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = sqrt(3), linetype = 2) + 
  geom_abline(intercept = 0, slope = sqrt(4), linetype = 2) +
  geom_abline(intercept = 0, slope = sqrt(3.5), linetype = 2, col = "blue")
  
df_for_plot %>% 
  ggplot(aes(x = value_monte_carlo, y = value_cubature, col = factor(s))) + 
  geom_point() + 
  scale_x_log10() + 
  scale_y_log10() + 
  geom_abline(intercept = .5*log10(3), slope = 1, linetype = 2) + 
  geom_abline(intercept = .5*log10(4), slope = 1, linetype = 2) 


df_for_plot %>% 
  mutate(ratio = value_cubature/value_monte_carlo) %>%  
  ggplot(aes(x = ratio)) + 
  facet_wrap(. ~ factor(s)) + 
  geom_histogram(alpha = .3) + 
  geom_vline(xintercept = c(sqrt(3), sqrt(4))) + 
  coord_cartesian(xlim = c(1.25,2.5))

lm(value_cubature ~ value_monte_carlo + as.factor(s), data = df_for_plot) %>% broom::tidy()

## So it is clear that 
  ## they are returning the same thing up to a constant. (that's good.) 
  ## the constant depends on s, which I think is an issue with the simulation side. 

## Another way to check 
# compare plurality to positional with s = 0.

plurality_positional_sim <- positional_piv_probs_simulation(gtools::rdirichlet(1000000, alpha = rep(1,6)), s = 0)

plurality_plurality_sim <- plurality_tie_probs_from_sims(gtools::rdirichlet(1000000, alpha = rep(2,3)))
# those two agree. 


plurality_positional <- positional_pivot_probs_general(s = 0, alpha = rep(1, 6))
plurality_plurality <- plurality_pivot_probs_dirichlet(alpha = rep(2, 3))
# these don't -- off by sqrt(2). 

## # if the integration stuff was correct, they would yield the same thing with s = 0. but they don't: the positional integration version is sqrt(2) times higher than the plurality integration one
unlist(map(plurality_positional, "integral"))/unlist(map(plurality_plurality, "integral"))

## also, the plurality integration one is sqrt(2) times higher than the simulation one. (this could be fine -- the channel is wider than we think.)
unlist(map(plurality_plurality, "integral"))/unlist(plurality_positional_sim)

## Note also that the correction relative to simulation depends on s: the positional integration result is 2x as high as the simulation when s = 0,1, but sqrt(3) times as high when s = .5. 

## Could this be because the correction we make to the Dirichlet density function to make it integrate to 1 is not correct? 
## the correction on the Dirichlet density is intended to make the integral be 1 over the whole unit simplex. the correction factor depends on the dimension (length) of alpha. when s = 0, we effectively have only 3 dimensions, so maybe the correction should actually be sqrt(3) instead of sqrt(6). But if we applied sqrt(3) the positional version would be even bigger, i.e. a bigger discrepancy.   

#  we could leave this as a mystery and work instead on the plurality comparison, or Condorcet. Maybe something will become clearer. 

```

we draw an $s$ 


### A Condorcet problem 

Now we seek to compute pivot probabilities for Condorcet methods. First, compute the probability of a decisive tie (i.e. one that does not result in a cycle) between each pair of candidates given `alpha` vector of `c(10, 5, 6,7, 3,9)`.

#### Direct Monte Carlo 

I wrote a function for computing Condorcet pivot probabilities from a set of simulations. 

I did some stuff to set the normalizing factor, but as noted above I am not sure if this makes sense or not. So I will skip for now. 


<!-- To set the normalizing factor, we again check to see how the probability of being in the crucial area depends on the parameter `tol`.   -->

<!-- ```{r fix_sims_condorcet, cache = T} -->
<!-- # draw simulations  -->
<!-- alpha_condorcet <- c(10, 5, 6,7, 3,9) -->
<!-- sims <- gtools::rdirichlet(1000000, alpha = alpha_condorcet) -->

<!-- pairwise_mat <- cbind(apply(sims[,c(1,2,5)], 1, sum), -->
<!--                       apply(sims[,c(1,2,3)], 1, sum), -->
<!--                       apply(sims[,c(1,3,4)], 1, sum)) -->

<!-- # function for a decisive tie within a window defined by tol -->
<!-- pr_in_window_condorcet <- function(tol, sims){ -->
<!--   mean(sims[,2] > 1/2 & # a beats c -->
<!--          sims[,3] > 1/2 & # b beats c    -->
<!--          abs(sims[,1] - 1/2) < tol/2) # a and b close  -->
<!-- } -->

<!-- # apply that function across several values of tol -->
<!-- data.frame(tol = seq(.005, .03, by = .005)) %>%  -->
<!--   mutate(piw = as.numeric(pmap(., pr_in_window_condorcet, sims = pairwise_mat))) -> df  -->
<!-- ``` -->

<!-- ```{r plot_condorcet_fix} -->
<!-- # plot the result along with a line of slope 1 and a line of slope sqrt(3) -->
<!-- df %>% ggplot(aes(x = tol, y = piw)) +  -->
<!--   geom_point() +  -->
<!--   geom_abline(slope = 1, intercept = 0, linetype = 2) +  -->
<!--   geom_abline(slope = sqrt(2), intercept = 0, linetype = 2, col = "yellow") +   geom_abline(slope = sqrt(3), intercept = 0, linetype = 2, col = "orange") + -->
<!--   geom_abline(slope = sqrt(6)/2, intercept = 0, linetype = 2, col = "red") -->

<!-- ``` -->


<!-- It seems to be $\sqrt{8}$ i.e. $2\sqrt{2}$. But the normalizer that makes the simulations and numerical results agree is $\frac{\sqrt{6}}{2}$. I need to clarify this.  -->

The function with no normalizing also computes Kemeny pivot probabilities, and we will use those below. 


```{r dmc_condorcet, cache = T}
# change
source("R/condorcet.R")
alpha_condorcet <- c(10, 5, 6,7, 3,9)
condorcet_sims <- gtools::rdirichlet(1000000, alpha_condorcet)
condorcet_dmc <- condorcet_pivot_probs_from_sims(condorcet_sims)
```


#### `SimplicialCubature` with generated `S` array 

We start by generating the `S` array for decisive Condorcet ties, i.e. those not resulting in a cycle: 

```{r decisive_condorcet_S, cache = T}
maxEvals <- 100000
condorcet_win_conditions <- rbind(
  c(1,1,-1,-1,1,-1),  # a beats b
  c(1,1,1,-1,-1,-1), # a beats c 
  c(1,-1,1,1,-1,-1)  # b beats c -- not a requirement for a to win, but this is a requirement for the a-b contest to be decisive
)
condorcet_S <- S_array_from_win_conditions(condorcet_win_conditions)
condorcet_out <- ordinal_shuffle_dirichlet_pivot_probs(alpha_condorcet, condorcet_S, tol = .1, maxEvals = maxEvals)
```

Did we hit the `maxEvals` ceiling? No:

```{r check_maxevals}
c(condorcet_out$ab$message, condorcet_out$ac$message, condorcet_out$bc$message)
```

#### Comparison

Now we compare the two approaches: 

```{r condorcet_comparison}
tibble(`Candidate pair` = names(condorcet_out), 
       `Monte Carlo` = c(condorcet_dmc$ab, condorcet_dmc$ac, condorcet_dmc$bc),
       `SimpCub integration` = c(condorcet_out$ab$integral,
                                 condorcet_out$ac$integral,
                                 condorcet_out$bc$integral)) %>% 
       mutate(`Ratio` = `SimpCub integration`/`Monte Carlo`) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```

Looks like the normalizing factor is $\sqrt(6)/2$. 

### Cyclical tie-breaking events: Kemeny with flat distribution 

Now we seek to compute pivot probabilities for a cycle-breaking procedure in which the winner is the one who loses by the least. For initial checking we'll set the  `alpha` parameter to `c(1, 1, 1, 1, 1, 1)`.

#### Direct Monte Carlo 

<!-- Let's check whether the normalizing factor is the same here:  -->

<!-- ```{r fix_sims_condorcet_kem, cache = T} -->
<!-- # draw simulations  -->
<!-- # ch -->
<!-- alpha_condorcet <- c(10, 5, 6,7, 3,9) -->
<!-- sims <- gtools::rdirichlet(1000000, alpha = alpha_condorcet) -->

<!-- pairwise_mat <- cbind(apply(sims[,c(1,2,5)], 1, sum), -->
<!--                       apply(sims[,c(1,2,3)], 1, sum), -->
<!--                       apply(sims[,c(1,3,4)], 1, sum)) -->

<!-- # function for a decisive tie within a window defined by tol -->
<!-- pr_in_window_condorcet_kemeny <- function(tol, sims){ -->
<!--   mean(# forward cycle -->
<!--     sims[,1] > 1/2 & # a beats b  -->
<!--       sims[,3] > 1/2 & # b beats c -->
<!--       sims[,2] < 1/2 & # # c beats a  -->
<!--     sims[,2] > 1 - sims[,3] & # a's score against c better than c's score against b -->
<!--       1 - sims[,1] > 1 - sims[,3] & # b's score against a better than c's score against b -->
<!--       abs(sims[,2] - (1 - sims[,1])) < tol/2) -->
<!-- } -->

<!-- # apply that function across several values of tol -->
<!-- data.frame(tol = seq(.001, .03, by = .0025)) %>%  -->
<!--   mutate(piw = as.numeric(pmap(., pr_in_window_condorcet_kemeny, sims = pairwise_mat))) -> df  -->
<!-- ``` -->

<!-- ```{r a_regression} -->
<!-- reg <- lm(log(piw) ~ log(tol), data = df) -->
<!-- broom::tidy(reg) %>% kable() %>% kable_styling(full_width = F) -->
<!-- ``` -->

<!-- ```{r plot_condorcet_fix_kem} -->
<!-- # plot the result along with a line of slope 1 and a line of slope sqrt(3) -->
<!-- df %>% ggplot(aes(x = tol, y = piw)) +  -->
<!--   geom_point() +  -->
<!--   geom_abline(slope = 1, intercept = 0, linetype = 2) +  -->
<!--   geom_abline(slope = 1/5, intercept = 0, linetype = 2, col = "green") +  -->
<!--   geom_abline(slope = 1/6, intercept = 0, linetype = 2, col = "yellow") + -->
<!--   geom_abline(slope = 1/7, intercept = 0, linetype = 2, col = "orange")  -->
<!-- ``` -->


Same procedure as above: 

```{r dmc_condorcet_1s, cache = T}
# chang
set.seed(12345)
alpha_condorcet <- rep(1, 6)
condorcet_sims <- gtools::rdirichlet(3000000, alpha_condorcet)
condorcet_dmc <- condorcet_pivot_probs_from_sims(condorcet_sims)
```


#### `SimplicialCubature` with generated `S` array 

We start by generating the `S` array for the forward cycle Kemeny case: 

```{r kemeny_condorcet_S, cache = F}
forward_cycle_conditions <- rbind(
  c(1,1,-1,-1,1,-1), # a beats b
  c(1,-1,1,1,-1,-1), # b beats c
  c(-1,-1,-1,1,1,1) # c beats a
  )
kemeny_ab_win_conditions_forward <- rbind(
  c(1,1,0,-1,0,-1),  # a's loss to c better than (equal to) b's loss to a
  c(1,0,1,0,-1,-1), # a's loss to c better than c's loss to b
  c(0,-1,1,1,-1,0),  # b's loss to a better than c's loss to b
  forward_cycle_conditions
)
kemeny_forward_S <- S_array_from_win_conditions(kemeny_ab_win_conditions_forward)
```

And the reverse one: 

```{r kemeny_condorcet_S_reverse, cache = F}
reverse_cycle_conditions <- -forward_cycle_conditions
kemeny_ab_win_conditions_reverse <- rbind(
  c(0,1,-1,-1,1,0),  # a's loss to b better than (equal to) b's loss to c
  c(1,1,0,-1,0,-1), # a's loss to b better than c's loss to a
  c(1,0,1,0,-1,-1),  # b's loss to c better than c's loss to a
  reverse_cycle_conditions
)
kemeny_reverse_S <- S_array_from_win_conditions(kemeny_ab_win_conditions_reverse)
```

Now we compute both:

```{r kemeny_compute, cache = T}
# ch
kemeny_forward_out <- ordinal_shuffle_dirichlet_pivot_probs(alpha_condorcet, kemeny_forward_S, tol = .1, maxEvals = maxEvals)
kemeny_reverse_out <- ordinal_shuffle_dirichlet_pivot_probs(alpha_condorcet, kemeny_reverse_S, tol = .1, maxEvals = maxEvals)
```

Did we hit the `maxEvals` ceiling? No:

```{r check_maxevals_kem}
c(kemeny_forward_out$ab$message, kemeny_forward_out$ac$message, kemeny_forward_out$bc$message, kemeny_reverse_out$ab$message, kemeny_reverse_out$ac$message, kemeny_reverse_out$bc$message)
```

Note that the `ordinal_shuffle` doesn't work quite the same way here, as the forward and reverse cycles are switched for candidate pair `ac`. I will switch it in the output below, but `general_numerical_methods.R` has a function `kemeny_pivot_probs_sc_based()` that does the computations and handles the swap.  

#### Comparison 


```{r kemeny_comparison_x}
tibble(`Candidate pair` = c("ab_forward", "ab_reverse", "ac_forward", "ac_reverse", "bc_forward", "bc_reverse"), 
       `Monte Carlo` = c(condorcet_dmc$ab_forward, condorcet_dmc$ab_reverse, condorcet_dmc$ac_forward, condorcet_dmc$ac_reverse, condorcet_dmc$bc_forward, condorcet_dmc$bc_reverse),
       `SimpCub integration` = c(kemeny_forward_out$ab$integral,
                                 kemeny_reverse_out$ab$integral,
                                 kemeny_reverse_out$ac$integral, # note swap
                                 kemeny_forward_out$ac$integral, # note swap
                                 kemeny_forward_out$bc$integral,
                                 kemeny_reverse_out$bc$integral)) %>% 
  mutate(`Ratio` = `SimpCub integration`/`Monte Carlo`) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```

It could be that the normalizing factor is slightly different for the two sets of results, but it could also be just simulation variation.  

### Cyclical tie-breaking events: Kemeny with another distribution 

Same thing, but now with other distributions to check how things are working. We'll set the  `alpha` parameter to `c(10, 4, 6, 7, 3, 12)`.

#### Direct Monte Carlo 

Same procedure as above: 

```{r dmc_condorcet_1s_x, cache = T}
# chan
set.seed(12345)
alpha_condorcet <- c(10, 4, 6, 7, 3, 12)
condorcet_sims <- gtools::rdirichlet(3000000, alpha_condorcet)
condorcet_dmc <- condorcet_pivot_probs_from_sims(condorcet_sims)
```


#### `SimplicialCubature` with generated `S` array 

We do the computations using the `S` array generated above:

```{r kemeny_compute_x, cache = T}
kemeny_forward_out <- ordinal_shuffle_dirichlet_pivot_probs(alpha_condorcet, kemeny_forward_S, tol = .1, maxEvals = maxEvals)
kemeny_reverse_out <- ordinal_shuffle_dirichlet_pivot_probs(alpha_condorcet, kemeny_reverse_S, tol = .1, maxEvals = maxEvals)
```

Did we hit the `maxEvals` ceiling? No:

```{r check_maxevals_kem_x}
c(kemeny_forward_out$ab$message, kemeny_forward_out$ac$message, kemeny_forward_out$bc$message, kemeny_reverse_out$ab$message, kemeny_reverse_out$ac$message, kemeny_reverse_out$bc$message)
```

#### Comparison 


```{r kemeny_comparison_xx}
tibble(`Candidate pair` = c("ab_forward", "ab_reverse", "ac_forward", "ac_reverse", "bc_forward", "bc_reverse"), 
       `Monte Carlo` = c(condorcet_dmc$ab_forward, condorcet_dmc$ab_reverse, condorcet_dmc$ac_forward, condorcet_dmc$ac_reverse, condorcet_dmc$bc_forward, condorcet_dmc$bc_reverse),
       `SimpCub integration` = c(kemeny_forward_out$ab$integral,
                                 kemeny_reverse_out$ab$integral,
                                 kemeny_reverse_out$ac$integral, # note swap
                                 kemeny_forward_out$ac$integral, # note swap
                                 kemeny_forward_out$bc$integral,
                                 kemeny_reverse_out$bc$integral)) %>% 
  mutate(`Ratio` = `SimpCub integration`/`Monte Carlo`) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```

The discrepancies here are a bit big (and consistent across different draws). The pattern of discrepancies suggests that the issue is with the simulations and not the numerical integration: `SimpCub` is too high for the forward cycles, but the `SimpCub` method is the same for `ab_forward`, `ac_reverse`, and `bc_forward`. 


### Cyclical tie-breaking events: Kemeny with yet another distribution 

Same thing, but now with other distributions to check how things are working. We'll set the  `alpha` parameter to `c(3, 9, 4, 10, 6, 6)`.

#### Direct Monte Carlo 

Same procedure as above: 

```{r dmc_condorcet_1s_xy, cache = T}
# change
alpha_condorcet <- c(3, 9, 4, 10, 6, 6)
condorcet_sims <- gtools::rdirichlet(1000000, alpha_condorcet)
condorcet_dmc <- condorcet_pivot_probs_from_sims(condorcet_sims)
```


#### `SimplicialCubature` with generated `S` array 

We do the computations using the `S` array generated above:

```{r kemeny_compute_xy, cache = T}
kemeny_forward_out <- ordinal_shuffle_dirichlet_pivot_probs(alpha_condorcet, kemeny_forward_S, tol = .1, maxEvals = maxEvals)
kemeny_reverse_out <- ordinal_shuffle_dirichlet_pivot_probs(alpha_condorcet, kemeny_reverse_S, tol = .1, maxEvals = maxEvals)
```


#### Comparison 


```{r kemeny_comparison_xxy}
tibble(`Candidate pair` = c("ab_forward", "ab_reverse", "ac_forward", "ac_reverse", "bc_forward", "bc_reverse"), 
       `Monte Carlo` = c(condorcet_dmc$ab_forward, condorcet_dmc$ab_reverse, condorcet_dmc$ac_forward, condorcet_dmc$ac_reverse, condorcet_dmc$bc_forward, condorcet_dmc$bc_reverse),
       `SimpCub integration` = c(kemeny_forward_out$ab$integral,
                                 kemeny_reverse_out$ab$integral,
                                 kemeny_reverse_out$ac$integral, # note swap
                                 kemeny_forward_out$ac$integral, # note swap
                                 kemeny_forward_out$bc$integral,
                                 kemeny_reverse_out$bc$integral)) %>% 
  mutate(`Ratio` = `SimpCub integration`/`Monte Carlo`) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```


## Next steps 

- wondering if there is a general approach for pivot probabilities; and why did this work for Borda Count? (Did it?)
- is Condorcet correct? looks like still some systematic deviations. 



### Diagnosing the issue 

I suspected there were points missing from the kemeny win region vertices.

```{r inspect_wrv}
wrv_forward <- cand_a_win_region_vertices_from_win_conditions(kemeny_ab_win_conditions_forward)
wrv_forward
wrv_reverse <- cand_a_win_region_vertices_from_win_conditions(kemeny_ab_win_conditions_reverse)
wrv_reverse
```

In particular, I thought $(1/6, 1/6, \ldots, 1/6)$ should be in both regions. But then I realized that this point is a linear combination of the 4th, 5th, and 6th rows of each matrix, so it's not needed in `wrv`. 

Now let's check the selection of vertices where the `binding_constraint` actually binds. This occurs in the function `simplices_to_integrate_from_win_region_vertices()`. 

```{r choosing_vertices}
binding_constraint_forward <- kemeny_ab_win_conditions_forward[1,]
wrv_forward %*% matrix(binding_constraint_forward, ncol = 1)

binding_constraint_reverse <- kemeny_ab_win_conditions_reverse[1,]
wrv_reverse %*% matrix(binding_constraint_reverse, ncol = 1)
```

That indeed is one vertex where the constraint does not bind: in the forward version, $a$ and $b$ tie but $b$ loses outright to $c$, so $a$ and $b$ cannot be tied in the tie-breaker. 

I looked through the process of producing this and it seems alright to me. So perhaps next I should check whether the simulation process has an error. It would be a symmetric error if so. 

Let's look at some extreme cases and see which approach gives more sensible answers. 

Here is a case where we expect a forward cycle. 
```{r fc_alpha}
alpha_condorcet <- c(10, 3, 2, 7, 10, 2)
```
It should yield high forward cycle pivot probabilities.

We compute via simulation:
```{r dmc_condorcet_1s_xx, cache = T}
# ch
condorcet_sims <- gtools::rdirichlet(1000000, alpha_condorcet)
condorcet_dmc <- condorcet_pivot_probs_from_sims(condorcet_sims)
```

We compute via `SC`: 

```{r kemeny_compute_xx, cache = T}
kemeny_forward_out <- ordinal_shuffle_dirichlet_pivot_probs(alpha_condorcet, kemeny_forward_S, tol = .1, maxEvals = maxEvals)
kemeny_reverse_out <- ordinal_shuffle_dirichlet_pivot_probs(alpha_condorcet, kemeny_reverse_S, tol = .1, maxEvals = maxEvals)
```

We compare:
```{r kemeny_comparison_xxx}
tibble(`Candidate pair` = c("ab_forward", "ab_reverse", "ac_forward", "ac_reverse", "bc_forward", "bc_reverse"), 
       `Monte Carlo` = c(condorcet_dmc$ab_forward, condorcet_dmc$ab_reverse, condorcet_dmc$ac_forward, condorcet_dmc$ac_reverse, condorcet_dmc$bc_forward, condorcet_dmc$bc_reverse),
       `SimpCub integration` = c(kemeny_forward_out$ab$integral,
                                 kemeny_reverse_out$ab$integral,
                                 kemeny_reverse_out$ac$integral, # note swwap
                                 kemeny_forward_out$ac$integral, # note swwap
                                 kemeny_forward_out$bc$integral,
                                 kemeny_reverse_out$bc$integral)) %>% 
  mutate(`Ratio` = `SimpCub integration`/`Monte Carlo`) %>% 
  kable() %>% 
  kable_styling(full_width = F)
```


<!-- ```{r kemeny_condorcet_S, cache = T} -->
<!-- maxEvals <- 100000 -->
<!-- condorcet_win_conditions_kemeny <- rbind( -->
<!--   c(1,1,-1,-1,1,-1),  # a beats b -->
<!--   c(1,1,1,-1,-1,-1) # a beats c  -->
<!-- ) -->
<!-- condorcet_S <- S_array_from_win_conditions(condorcet_win_conditions) -->
<!-- condorcet_out <- ordinal_shuffle_dirichlet_pivot_probs(alpha_condorcet, condorcet_S, tol = .1, maxEvals = maxEvals) -->
<!-- ``` -->













## Next steps 

Now that I have things agreeing, I want to go back to my `general_numerical_methods.R` methods applying what I have learned here. Start by doing plurality with 5 or 6 candidates, then Borda count and Condorcet. I believe I have a nice general powerful method that is unfortunately very slow for interesting problems. But I need to check that it works (now that I've fixed a few things), compare performance to simulation, and see if I can speed it up a little. That will be fun and absorbing. 


## Scrap below this 


As is the ratio for the same quantity for each of the other candidates:

```{r ratio_check_2, cache = T}
b_50 <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(.5, .5, 0), c(0, .5, .5), c(0,1,0)), alpha = alpha)
c_50 <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(0, .5, .5), c(.5, 0, .5), c(0,0,1)), alpha = alpha)

sc_50s <- c(sc_50$integral, b_50$integral, c_50$integral)
dmc_50s <- c(mean(sims[,1] > .5), mean(sims[,2] > .5), mean(sims[,3] > .5))

sc_50s/dmc_50s

```

We can actually match the simulation-based result with `SimplicialCubature` if we drop a dimension:
```{r sc_50_2, cache = T, error = T}
S_50_2 <- cbind(c(.5, .5), c(.5, 0), c(1,0))
dir_fn2 <- function(x, alpha){gtools::ddirichlet(c(x, 1 - sum(x)), alpha = alpha)}
sc_50_2 <- SimplicialCubature::adaptIntegrateSimplex(dir_fn2, S = S_50_2, alpha = alpha)
sc_50_2$integral
```


### Diagnosis 

What explains the different results? Let's see how the differences vary across the three possible ties we could have in the same election. 

Here is the probability of each tie according to direct Monte Carlo: 

```{r three_sims, cache = T}
ab_tie_for_first <- function(sims, tol = .01){
  row_max <- apply(sims, 1, max)
  mean((sims[,1] == row_max | sims[,2] == row_max) & abs(sims[,1] - sims[,2]) < tol/2)/(tol/sqrt(2))  
}

dmc_3 <- c(ab_tie_for_first(sims), ab_tie_for_first(sims[,c(1,3,2)]), ab_tie_for_first(sims[,c(2,3,1)]))
dmc_3
```

And here are the three `SimplicialCubature`-based results: 

```{r three_scs, cache = T}
sc_result_ac <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(.5, 0, .5), c(1/3, 1/3, 1/3)), alpha = alpha)
sc_result_bc <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(.0, .5, .5), c(1/3, 1/3, 1/3)), alpha = alpha)

sc_3 <- c(sc_result$integral, sc_result_ac$integral, sc_result_bc$integral)
sc_3

sc_3/dmc_3
```

Same comparison but for different parameters on the integrand function: 

```{r diff_alpha, cache = T}
# ch 
alpha2 <- c(18, 15, 8)
sc_result_ab <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(.5, .5, 0), c(1/3, 1/3, 1/3)), alpha = alpha2)
sc_result_ac <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(.5, 0, .5), c(1/3, 1/3, 1/3)), alpha = alpha2)
sc_result_bc <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(.0, .5, .5), c(1/3, 1/3, 1/3)), alpha = alpha2)

sc_3a <- c(sc_result_ab$integral, sc_result_ac$integral, sc_result_bc$integral)
sc_3a

sims2 <- gtools::rdirichlet(N, alpha2)
dmc_3a <- c(ab_tie_for_first(sims2), ab_tie_for_first(sims2[,c(1,3,2)]), ab_tie_for_first(sims2[,c(2,3,1)]))

dmc_3a 

sc_3a/dmc_3a
```

So the `SimplicialCubature` results are consistently around $\sqrt{3}$ times bigger than the simulation-based results. What explains this?

I think the confusion arises because distances are spread out on the unit simplex compared to its projection onto $\mathbb{R}^{k-1}$. We can see this in the numerical integration approach, where we are integrating along a line on the unit simplex.  Let one point along the line be $(x,y,z)$ and let the next point be $(x + a, y + a, z - 2a)$. Then the distance between these points is $\sqrt{(a^2 + a^2 + 4a^2)} = \sqrt{6a^2} \approx 2.45a$. Initially I did not see this and treated my step size as $a$. My simulation answer and grid-based numerical integration answer agreed, but they differed from the `SimplicialCubature` answer by that amount. Then I corrected the step size for the numerical integration and that agreed with `SimplicialCubature`. Next I noticed that the width of the target area in the Monte Carlo was not `tol` but `tol/sqrt(2)` But I don't yet see how I get the remaining $\sqrt{3}$ in the simulation approach: if the Dirichlet draws are being done correctly, then I think I should get the correct answer by checking how often the result is close to the pivot event, as long as I am correct about what "close" means.        

Here is a slightly different function for the direct Monte Carlo: 

```{r three_sims_2, cache = T}
# ch
ab_tie_for_first_v2 <- function(sims, tol = .01){
  mean(sims[,1] == apply(sims, 1, max) & sims[,1] - sims[,2] < tol)/(tol/sqrt(2))  
}

dmc_4 <- c(ab_tie_for_first_v2(sims), ab_tie_for_first_v2(sims[,c(1,3,2)]), ab_tie_for_first_v2(sims[,c(2,3,1)]))
dmc_4
```

But it produces the same results, right? 

```{r identity_check}
dmc_3/dmc_4
```

Are the results that I identify in the direct Monte Carlo within a band of width `tol` along the tie line?

Maybe make a plot? 

```{r plot_mc_region, webgl=T}
open3d()
points3d(x = c(1,0,0), y = c(0,1,0), z = c(0,0,1), pch = 19)
lines3d(x = c(1,0,0, 1), y = c(0,1,0,0), z = c(0, 0,1,0), col = "black")
ax_val <- 1.25
ax_col <- rgb(.1, .1, .1, alpha = .5)
lines3d(x = c(0,ax_val), y = c(0,0), z = c(0,0), col = ax_col)
lines3d(x = c(0,0), y = c(0,ax_val), z = c(0,0), col = ax_col)
lines3d(x = c(0,0), y = c(0,0), z = c(0,ax_val), col = ax_col)

# plot some points on the simplex, coloring them according to whether they would be considered a tie or not
sims <- gtools::rdirichlet(1000, alpha = c(10, 10, 3))
a <- .025
in_region <- sims[,1] > sims[,2] & sims[,1] > sims[,3] & sims[,1] - sims[,2] < a
cols <- c(rgb(.2, .2, .2, alpha = .5), rgb(.8, .2, .2, alpha = .5))
points3d(sims, pch = 19, cex = .5, col = cols[1 + as.integer(in_region)])

# try to bound that area in box 
planes3d(a = 1, col = rgb(.5, .5, .5, alpha = .5))
lines3d(x = c(1/2, 1/3), y = c(1/2, 1/3), z = c(0, 1/3), col = "blue", lty = 2)
lines3d(x = c(1/2 + a/2, 1/3 + a/2), y = c(1/2-a/2, 1/3-a/2), z = c(0, 1/3), col = "red", lty = 2)

# two points on the edges 
z <- .1
points3d(x = (1 - z)/2, y = (1 - z)/2, z = z, pch = 19, size = 5, col = "green")
points3d(x = (1 - z)/2 + a/2, y = (1 - z)/2 - a/2, z = z, pch = 19, size = 5, col = "green")
# those points are on the edges of the region. 
# key point: the distance between those points is not a, as it may first appear! 
# it is a/sqrt(2).

```


### A different problem 

Here is a different problem that involves integrating an area rather than integrating along a line: what is the probability that candidate 1 wins more than 50\% of the vote? 

But we can't match the tie probabilities this way: 
```{r ab_tie_2, cache = T, error = T}
sc_ab <- SimplicialCubature::adaptIntegrateSimplex(dir_fn2, S = cbind(c(.5, .5), c(1/3, 1/3)), alpha = alpha)
sc_ac <- SimplicialCubature::adaptIntegrateSimplex(dir_fn2, S = cbind(c(.5, 0), c(1/3, 1/3)), alpha = alpha)
sc_bc <- SimplicialCubature::adaptIntegrateSimplex(dir_fn2, S = cbind(c(.0, .5), c(1/3, 1/3)), alpha = alpha)

sc_2d <- c(sc_ab$integral, sc_ac$integral, sc_bc$integral)
sc_2d
```

The relationship between these tie probabilities and the simulation-based probabilities is not linear: 

```{r compare_sc_2d_and_sim}
sc_2d/dmc_3
```

But now note that the integration over the unit simplex yields a result of $\sqrt{3}$: 

```{r total_integral, cache = T}
totes <- SimplicialCubature::adaptIntegrateSimplex(dir_fn, S = cbind(c(1, 0, 0), c(0, 1, 0), c(0,0,1)), alpha = alpha)
totes$integral
```

Hmm, in that case is the integral on the unit simplex really correct? It seems to me that I have learned that I was not doing the Monte Carlo correctly (missing the $\sqrt{2}$) but the $\sqrt{3}$ difference is not appropriate. 


I have assumed that the simulation-based results are the target, so it seems like my use of `SimplicialCubature` must be incorrect.  



### Suggestion for `SimplicialCubature`

I encountered an issue trying to use `gtools::ddirichlet` as my integrand function, and I wondered if John might want to fix something in the `SimplicialCubature` code to address it. 

To highlight the issue, let's integrate a function on the unit simplex. This works fine:

```{r examp_sum, error = T}
S <- cbind(c(1,0,0), c(0,1,0), c(0,0,1)) # same as UnitSimplexV(3)
sum_fn <- function(x){sum(x)}
SimplicialCubature::adaptIntegrateSimplex(sum_fn, S)
```

But we encounter an error when we replace the integrand function with a Dirichlet density: 

```{r examp_dir, error = T}
dir_fn_naive <- function(x, alpha){
  gtools::ddirichlet(x = x, alpha = alpha)
}
SimplicialCubature::adaptIntegrateSimplex(dir_fn_naive, S, alpha = c(1,1,1))
```

`gtools::ddirichlet()` seems to think it's being given an `x` argument that is not of the same length as `alpha` (3). 

The problem seems to be that `x` is a matrix, and `gtools::ddirichlet()` is expecting a vector. If we change the format explicitly, no error: 
<!-- I can fix the problem if I put the elements of `x` together explicitly:  -->

```{r examp_dir_fix, error = T}
dir_fn_fixed <- function(x, alpha){
  gtools::ddirichlet(x = as.vector(x), alpha = alpha)
}
SimplicialCubature::adaptIntegrateSimplex(dir_fn_fixed, S, alpha = c(1,1,1))
```

We could consider altering  John alter the code (presumably `adsimp()` and `integrate.vector.fn()`) to work with integrand functions that expect `x` to be a vector? 




## Generating the `S` matrix: John's code and my code (my notes, really)

John kindly sent me some code for generating the `S` matrix given some vertices. Here I load his code and run an example he sent along:  

```{r load_code, webgl=T}
source('R/TesselationsOfPointCloud.R')
library(mvmesh)
V <- cbind( c(1,0,1e-20), c(0,1,0), c(0,0,1), c(.25,.5,.25), c(.1,.4,.5) )
V
S <- simplicesFromVertices( V )
S

open3d()
  points3d( t(V), size=10,col="red" )
  for (i in 1:dim(S)[3]) {
    wrap <- cbind( S[,,i], S[,1,i] )
    lines3d(t(wrap), width=3, col='blue')
  }
```

This code is useful for taking a set of vertices and tesselating it, i.e. rendering a bunch of simplices in the form of an `S` matrix that we can then pass to the `adaptIntegrateSimplex` function. 

The approach I wrote up is based on determining the convex hull of a set of vertices and then extracting the facets relevant to my problem. The `geometry::convhulln()` function represents the convex hull as triangulated/tesselated combinations of vertices, so my function takes the whole set of vertices for a candidate's "win region", get the triangulated/tesselated convex hull, and identifies those facets where a certain condition binds (e.g. $a$ tying $b$ in a particular way) at all vertices; these are the facets where I need to integrate my function. 

In short, my code is more specific to my problem, but like John's code it involves dropping a dimension (projecting down to $\mathbb{R}^{k-1}$) and tesselating.     
